{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers, models\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (train_images.shape, train_labels.shape))\n",
    "print('Test: X=%s, y=%s' % (test_images.shape, test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first few images\n",
    "for i in range(1):\n",
    "    # define subplot\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    # plot raw pixel data\n",
    "    plt.imshow(train_images[i], cmap=plt.get_cmap('gray'))\n",
    "    print(train_labels[i])\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Angel Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angel_model = models.Sequential()\n",
    "angel_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "angel_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "angel_model.add(layers.AveragePooling2D((2, 2)))\n",
    "angel_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "angel_model.add(layers.Flatten())\n",
    "angel_model.add(layers.Dense(64, activation='relu'))\n",
    "angel_model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angel_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angel_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "angel_history = angel_model.fit(train_images, train_labels, batch_size=32, epochs=15,\n",
    "                    callbacks=callback, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Angel's Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(angel_history.history['accuracy'], label='accuracy')\n",
    "plt.plot(angel_history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.96, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = angel_model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "angel_model.save('angel_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### John Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_model = models.Sequential()\n",
    "john_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "john_model.add(layers.AveragePooling2D((2, 2)))\n",
    "john_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "john_model.add(layers.AveragePooling2D((2, 2)))\n",
    "john_model.add(layers.Flatten())\n",
    "john_model.add(layers.Dense(64, activation='relu'))\n",
    "john_model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "john_history = john_model.fit(train_images, train_labels, batch_size=32, epochs=15,\n",
    "                    callbacks=callback, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate John's Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(john_history.history['accuracy'], label='accuracy')\n",
    "plt.plot(john_history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.96, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = john_model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_model.save('john_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justin Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justin_model = models.Sequential()\n",
    "justin_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "justin_model.add(layers.Dropout(0.2))\n",
    "justin_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "justin_model.add(layers.AveragePooling2D((2, 2)))\n",
    "justin_model.add(layers.Flatten())\n",
    "justin_model.add(layers.Dense(64, activation='relu'))\n",
    "justin_model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justin_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justin_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "justin_history = justin_model.fit(train_images, train_labels, batch_size=32, epochs=15,\n",
    "                    callbacks=callback, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(justin_history.history['accuracy'], label='accuracy')\n",
    "plt.plot(justin_history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.96, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = justin_model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justin_model.save('justin_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models\n",
    "\n",
    "Here we load models into single ensemble model as list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['angel_model', 'john_model', 'justin_model']\n",
    "trained_models = {}\n",
    "\n",
    "for name in model_names:\n",
    "    if os.path.isdir(name):\n",
    "        trained_models[name] = models.load_model(name)\n",
    "    else:\n",
    "        print(f\"Invalid model name {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 5\n",
    "for _ in range(runs):\n",
    "\n",
    "    random_index = random.randint(0, len(test_images) -1)\n",
    "    random_image = test_images[random_index]\n",
    "    image_label = test_labels[random_index]\n",
    "    model_predictions = [None] * len(trained_models.keys())\n",
    "    ensemble_predictions = {}\n",
    "\n",
    "    # Use each individual model to predict\n",
    "    for i, (name, model) in enumerate(trained_models.items()):\n",
    "        model_predictions[i] = np.argmax(model.predict(tf.reshape(random_image, shape=[1, 28, 28, 1])))\n",
    "\n",
    "    # Vote on final output\n",
    "    for pred in model_predictions:\n",
    "        if pred in ensemble_predictions.keys():\n",
    "            ensemble_predictions[pred] += 1\n",
    "        else:\n",
    "            ensemble_predictions[pred] = 1\n",
    "    \n",
    "    print(f'True label:\\t\\t{image_label}')\n",
    "\n",
    "    # All models agree\n",
    "    if len(ensemble_predictions) == 1:\n",
    "        print(f'Ensemble pred. label:\\t{list(ensemble_predictions.keys())[0]}')\n",
    "    \n",
    "    # 1 model disagrees\n",
    "    elif len(ensemble_predictions) == 2:\n",
    "        print(f'Ensemble pred. label:\\t{max(ensemble_predictions, key=ensemble_predictions.get)}')\n",
    "\n",
    "    # all models disagree\n",
    "    # need to return model with best track record\n",
    "    else:\n",
    "        random_pick = random.choice(list(ensemble_predictions.keys()))\n",
    "        print(f'Ensemble pred. label:\\t{random_pick}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating KKanji Datasets (Midterm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load datasets into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_kkanji_midterm_dataset_train = tf.keras.utils.image_dataset_from_directory(\n",
    "                                        './datasets/midterm_dataset/',\n",
    "                                        validation_split=0.3,\n",
    "                                        subset=\"training\",\n",
    "                                        seed=132,\n",
    "                                        image_size=(64, 64),\n",
    "                                        batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_kkanji_midterm_dataset_val = tf.keras.utils.image_dataset_from_directory(\n",
    "                                        './datasets/midterm_dataset/',\n",
    "                                        validation_split=0.3,\n",
    "                                        subset=\"validation\",\n",
    "                                        seed=132,\n",
    "                                        image_size=(64, 64),\n",
    "                                        batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_names = new_kkanji_midterm_dataset_train.class_names\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in new_kkanji_midterm_dataset_train.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in new_kkanji_midterm_dataset_train:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data rescaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "normalized_ds = new_kkanji_midterm_dataset_train.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Performance Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = new_kkanji_midterm_dataset_train.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = new_kkanji_midterm_dataset_train.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE the change in input/output dimensions from the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angel_new_model = models.Sequential()\n",
    "angel_new_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "angel_new_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "angel_new_model.add(layers.AveragePooling2D((2, 2)))\n",
    "angel_new_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "angel_new_model.add(layers.Flatten())\n",
    "angel_new_model.add(layers.Dense(64, activation='relu'))\n",
    "angel_new_model.add(layers.Dense(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angel_new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angel_new_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "angel_history = angel_new_model.fit(new_kkanji_midterm_dataset_train, epochs=15,\n",
    "                    callbacks=callback, validation_data=new_kkanji_midterm_dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(angel_history.history['accuracy'], label='accuracy')\n",
    "plt.plot(angel_history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.75, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = angel_new_model.evaluate(new_kkanji_midterm_dataset_val, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angel_new_model.save('ange_kanji_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### John Test Kanji Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_kanji_model = models.Sequential()\n",
    "john_kanji_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "john_kanji_model.add(layers.AveragePooling2D((2, 2)))\n",
    "john_kanji_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "john_kanji_model.add(layers.AveragePooling2D((2, 2)))\n",
    "john_kanji_model.add(layers.Flatten())\n",
    "john_kanji_model.add(layers.Dense(64, activation='relu'))\n",
    "john_kanji_model.add(layers.Dense(50))\n",
    "\n",
    "john_kanji_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "justin_history = john_kanji_model.fit(new_kkanji_midterm_dataset_train, epochs=15,\n",
    "                    callbacks=callback, validation_data=new_kkanji_midterm_dataset_val)\n",
    "\n",
    "john_kanji_model.save('john_kanji_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Justin Test Kanji Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justin_kanji_model = models.Sequential()\n",
    "justin_kanji_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "justin_kanji_model.add(layers.Dropout(0.2))\n",
    "justin_kanji_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "justin_kanji_model.add(layers.AveragePooling2D((2, 2)))\n",
    "justin_kanji_model.add(layers.Flatten())\n",
    "justin_kanji_model.add(layers.Dense(64, activation='relu'))\n",
    "justin_kanji_model.add(layers.Dense(50))\n",
    "\n",
    "justin_kanji_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "justin_history = justin_kanji_model.fit(new_kkanji_midterm_dataset_train, epochs=15,\n",
    "                    callbacks=callback, validation_data=new_kkanji_midterm_dataset_val)\n",
    "\n",
    "justin_kanji_model.save('justin_kanji_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Midterm Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Kanji Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_kkanji_midterm_dataset_train = tf.keras.utils.image_dataset_from_directory(\n",
    "                                        './datasets/midterm_dataset/',\n",
    "                                        validation_split=0.3,\n",
    "                                        subset=\"training\",\n",
    "                                        seed=132,\n",
    "                                        image_size=(64, 64),\n",
    "                                        batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_kkanji_midterm_dataset_val = tf.keras.utils.image_dataset_from_directory(\n",
    "                                        './datasets/midterm_dataset/',\n",
    "                                        validation_split=0.3,\n",
    "                                        subset=\"validation\",\n",
    "                                        seed=132,\n",
    "                                        image_size=(64, 64),\n",
    "                                        batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Illustrate subsample of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = new_kkanji_midterm_dataset_train.class_names\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for images, labels in new_kkanji_midterm_dataset_train.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo with small subsample from validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from angel_code.ensemble_classifier import EnsembleClassifier\n",
    "\n",
    "ensemble_model = EnsembleClassifier('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model.demo(new_kkanji_midterm_dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import optuna\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load datasets into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_kkanji_midterm_dataset_train = tf.keras.utils.image_dataset_from_directory(\n",
    "                                        './datasets/midterm_dataset/',\n",
    "                                        validation_split=0.3,\n",
    "                                        subset=\"training\",\n",
    "                                        seed=132,\n",
    "                                        image_size=(64, 64),\n",
    "                                        batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_kkanji_midterm_dataset_val = tf.keras.utils.image_dataset_from_directory(\n",
    "                                        './datasets/midterm_dataset/',\n",
    "                                        validation_split=0.3,\n",
    "                                        subset=\"validation\",\n",
    "                                        seed=132,\n",
    "                                        image_size=(64, 64),\n",
    "                                        batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data rescaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "normalized_ds = new_kkanji_midterm_dataset_train.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Performance Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = new_kkanji_midterm_dataset_train.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = new_kkanji_midterm_dataset_train.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # The following is where you suggest your search space per trial\n",
    "    # You can suggest integer, categorical and floating point values\n",
    "    # The following lines may change according to your model architecture\n",
    "    first_layer_kernel = trial.suggest_int('first_layer_kernel', 8, 64)\n",
    "    first_layer_activation = trial.suggest_categorical('first_layer_activation', ['relu', 'sigmoid', 'tanh'])\n",
    "    second_layer_kernel = trial.suggest_int('second_layer_kernel', 8, 64)\n",
    "    second_layer_activation = trial.suggest_categorical('second_layer_activation', ['relu', 'sigmoid', 'tanh'])\n",
    "    average_pooling_size = trial.suggest_int('average_pooling_size', 1, 4)\n",
    "    third_layer_kernel = trial.suggest_int('third_layer_kernel', 8, 64)\n",
    "    third_layer_activation = trial.suggest_categorical('third_layer_activation', ['relu', 'sigmoid', 'tanh'])\n",
    "    dense_layer_size = trial.suggest_int('dense_layer_size', 4, 128)\n",
    "    dense_layer_activation = trial.suggest_categorical('dense_layer_activation', ['relu', 'sigmoid', 'tanh'])\n",
    "    \n",
    "    # Based on the optuna variables returned you can build a new architecture\n",
    "    # I don't think you'll need to change the input/output layers\n",
    "    # Really the focus should be on the kernel/dense layer shapes and your activation functions (if you want)\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(first_layer_kernel, (3, 3), activation=first_layer_activation, input_shape=(64, 64, 3)))\n",
    "    model.add(layers.Conv2D(second_layer_kernel, (3, 3), activation=second_layer_activation))\n",
    "    model.add(layers.AveragePooling2D((average_pooling_size, average_pooling_size)))\n",
    "    model.add(layers.Conv2D(third_layer_kernel, (3, 3), activation=third_layer_activation))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dense_layer_size, activation=dense_layer_activation))\n",
    "    model.add(layers.Dense(50))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "    history = model.fit(train_ds, epochs=15,\n",
    "                    callbacks=callback, validation_data=val_ds)\n",
    "\n",
    "    # Following line is very important as this is the metric that optuna uses to optimize over\n",
    "    return history.history['val_accuracy'][-1]\n",
    "\n",
    "# I found that 50 for n_trials MIGHT be over kill, it found the optimal config at ~15 trials in\n",
    "# Feel free to adjust as you see fit and judging on how your machine is doing.\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Trial Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best trial info:\\n{study.best_trial}\\n')\n",
    "for param, value in study.best_params.items():\n",
    "    print(f'Param: {param}\\tValue: {value}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('brush_strokes')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "425c80cf2732746be3b317b687ca1852eb62c983ce48210a60c1f5072c461cc8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
